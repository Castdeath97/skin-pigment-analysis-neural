{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npandas.core.series.Series: Lesion types (text) series sorted by idx for labels\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as met\n",
    "\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "BASE_PROCESSED_DATA_DIR = '../data/processed'\n",
    "\"\"\"\n",
    "str: Base processed data directory\n",
    "\"\"\"\n",
    "\n",
    "PROCESSED_CSV_FILE = BASE_PROCESSED_DATA_DIR + '/processed.csv'\n",
    "\"\"\"\n",
    "str: HAM1000_metadata.csv metadata file location \n",
    "\"\"\"\n",
    "        \n",
    "# Read dataset in\n",
    "skin_df = pd.read_csv(PROCESSED_CSV_FILE, index_col=0)\n",
    "\"\"\"\n",
    "pandas.core.frame.DataFrame: final dataset\n",
    "\"\"\"\n",
    "\n",
    "def printMetrics(prediction, y_test):\n",
    "    \"\"\"\n",
    "    Prints accuracy, confusion and F1 metrics\n",
    "    \n",
    "    returns list of accuracy, confusion and F1 metrics\n",
    "    \"\"\"\n",
    "    accuracy = met.accuracy_score(y_test, prediction)\n",
    "    confusion = met.confusion_matrix(y_test, prediction)\n",
    "    f1_score_avg = met.f1_score(y_test, prediction, average='weighted')\n",
    "    f1_score = met.f1_score(y_test, prediction, average= None)\n",
    "\n",
    "    print('accuracy', accuracy)\n",
    "    print()\n",
    "    print(confusion)\n",
    "    print()\n",
    "    print('f1 average: ', f1_score_avg)\n",
    "    print('f1: ', f1_score)\n",
    "\n",
    "    return([accuracy, confusion, f1_score_avg])\n",
    "\n",
    "lesion_type_label = skin_df[\n",
    "    ['lesion_type_idx', 'lesion_type']].sort_values(\n",
    "    'lesion_type_idx').drop_duplicates()['lesion_type']\n",
    "\"\"\"\n",
    "pandas.core.series.Series: Lesion types (text) series sorted by idx for labels\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Introduction\n",
    "\n",
    "This report documents the process of creating neural networks to model skin pigment diagnosis and the evaluation of said models (especially compared to other methods used in the previous analysis). A basic Sequential neural network will be created.\n",
    "\n",
    "\n",
    "#### One Hot Encoding and Minor Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical cols using one hot encoding\n",
    "\n",
    "one_hot_localization = pd.get_dummies(skin_df['localization'])\n",
    "one_hot_localization.drop('unknown', axis=1, inplace = True)\n",
    "\n",
    "one_hot_sex = pd.get_dummies(skin_df['sex'])\n",
    "one_hot_sex.drop('unknown', axis=1, inplace = True)\n",
    "\n",
    "# Drop old categorical cols and replace with new ones\n",
    "# drop dx type (not needed beyond data understanding)\n",
    "\n",
    "skin_df.drop(['dx_type', 'localization', 'sex'], axis = 1, inplace = True)\n",
    "\n",
    "# Join the encoded dfs\n",
    "\n",
    "skin_df = skin_df.join(one_hot_localization)\n",
    "skin_df = skin_df.join(one_hot_sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas dummies for categorical variables, localization values are one hot coded using new columns for every value (0 false / 1 true), however one of the columns is dropped since a negation of all the other columns represents it. Lastly, the now redundant sex and localization fields are dropped alongside dx_type (no need for analysing diagnosis type beyond Data Understanding).\n",
    "\n",
    "#### Test Split and Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  if sys.path[0] == '':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  del sys.path[0]\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and test data in a 50-50 split\n",
    "# Don't include lesion_types (used for response) and image path (not used yet)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    skin_df.drop(['lesion_type_idx', 'lesion_type'], axis=1),\n",
    "    skin_df['lesion_type_idx'], test_size=0.5, random_state=0)\n",
    "\n",
    "# scale using a partial fit for speed\n",
    "\n",
    "scaling = StandardScaler()\n",
    "\n",
    "scaling.partial_fit(X_test)\n",
    "X_test = scaling.transform(X_test)\n",
    "\n",
    "scaling.partial_fit(X_train)\n",
    "X_train = scaling.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data and the testing data are separated using a 50-50 split respectively, both sets consist of a set of predictors (X) and a response (y). The predictor data has the lesion_type_idx and lesion_type fields removed since they can leak the ground truth. For the response data only the lesion_type_idx field is used since it is sufficient at representing the category of skin lesion (the response / what is being predicted).\n",
    "\n",
    "To ensure that the impact of predictors is not effected by the measurement scale - which could occur in this dataset due to the variety of predictors - the predictors are scaled using a scaling transform (i.e. with default mean and standard deviation transform).\n",
    "\n",
    "### Measurements\n",
    "\n",
    "The computer used to carry the measurements has the following specifications:\n",
    "* CPU: i7-7700HQ\n",
    "* RAM: 8GB\n",
    "* OS: Windows 10\n",
    "* GPU: GTX 1060 (notebook)\n",
    "\n",
    "To evaluate the model the following measurements are taken:\n",
    "* Fit time: Using the time python library, a timer is started and stopped to measure tuning and fit .\n",
    "* Prediction time: Using the time python library, a timer is started and stopped to measure the prediction.\n",
    "* Confusion matrix: Using the sklearn metrics library a confusion matrix is printed.\n",
    "* F1 Score: Using the sklearn metrics library a F1 score is calculated using weighted averages and for every class.\n",
    "\n",
    "## Models Description and Assessments\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "Neural networks are machine learning tools that are inspired by human brain biology, relying on multiple connected neurons that interact with one another through multiple layers. Neural networks emphasize the importance of interaction between features more so than traditional models reliant on basic sums of coefficients attached to predictors.\n",
    "\t\t\t\t\n",
    "#### Sequential Neural Network\n",
    "\n",
    "##### Introduction \t\t\n",
    "\n",
    "Sequential Neural Networks layers are sequentially attached meaning each layer's neurons only attach to the next layer's. \n",
    "\n",
    "##### Construction and Tuning \n",
    "\n",
    "The networks will use relu activation - but softmax at final layer - and Adam optimisation using categorical cross entropy loss. The model training will early stop if no improvement occurs after 3 iterations via early stopping callback.\n",
    "\n",
    "To help reduce overfitting a dropout layer of proportion 0.1 will be added between layers (chooses a random subset of units in the layer to ignore in propagation steps).\n",
    "\n",
    "To tune the networks multiple models of increasing complexity/capacity (various widths and/or layer counts) are created and the ones with the best performance are picked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to separate targets\n",
    "target = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq_model(hidden_layers, nodes_per_layer) :\n",
    "    \"\"\"\n",
    "    creates a sequential neural networks with a set of hidden \n",
    "    layers (hidden_layers) followed by 0.1 drop out\n",
    "    with a given number of nodes (nodes_per_layer)\n",
    "    which is complied in a 0.3 validation split in 20 epochs (early stop 3)\n",
    "    \n",
    "    returns keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    early_stopping_monitor = EarlyStopping(patience = 3)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    for i in range(hidden_layers - 1) :\n",
    "        if i == 0 :\n",
    "            model.add(Dense(nodes_per_layer, activation = 'relu', input_shape = (X_train.shape[1],)))\n",
    "        else :\n",
    "            model.add(Dense(nodes_per_layer, activation = 'relu'))\n",
    "        model.add(Dropout(0.10))\n",
    "\n",
    "    \n",
    "    model.add(Dense(7, activation = 'softmax'))\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "                   metrics = ['accuracy'])\n",
    "    model.fit(X_train, target, validation_split = 0.3, epochs = 20,\n",
    "               callbacks = [early_stopping_monitor])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3504 samples, validate on 1503 samples\n",
      "Epoch 1/20\n",
      "3504/3504 [==============================] - 1s 373us/step - loss: 1.8989 - acc: 0.4538 - val_loss: 1.7017 - val_acc: 0.5210\n",
      "Epoch 2/20\n",
      "3504/3504 [==============================] - 1s 183us/step - loss: 1.5707 - acc: 0.5285 - val_loss: 1.5470 - val_acc: 0.5529\n",
      "Epoch 3/20\n",
      "3504/3504 [==============================] - 1s 182us/step - loss: 1.4630 - acc: 0.5736 - val_loss: 1.4654 - val_acc: 0.5396\n",
      "Epoch 4/20\n",
      "3504/3504 [==============================] - 1s 181us/step - loss: 1.3055 - acc: 0.6002 - val_loss: 1.4313 - val_acc: 0.5522\n",
      "Epoch 5/20\n",
      "3504/3504 [==============================] - 1s 183us/step - loss: 1.2136 - acc: 0.6247 - val_loss: 1.2863 - val_acc: 0.6035\n",
      "Epoch 6/20\n",
      "3504/3504 [==============================] - 1s 191us/step - loss: 1.1167 - acc: 0.6667 - val_loss: 1.3116 - val_acc: 0.5941\n",
      "Epoch 7/20\n",
      "3504/3504 [==============================] - 1s 185us/step - loss: 1.1122 - acc: 0.6632 - val_loss: 1.2803 - val_acc: 0.6081\n",
      "Epoch 8/20\n",
      "3504/3504 [==============================] - 1s 181us/step - loss: 1.0194 - acc: 0.6909 - val_loss: 1.1288 - val_acc: 0.6534\n",
      "Epoch 9/20\n",
      "3504/3504 [==============================] - ETA: 0s - loss: 0.9401 - acc: 0.721 - 1s 178us/step - loss: 0.9463 - acc: 0.7169 - val_loss: 1.2593 - val_acc: 0.6174\n",
      "Epoch 10/20\n",
      "3504/3504 [==============================] - 1s 190us/step - loss: 0.9078 - acc: 0.7172 - val_loss: 1.2388 - val_acc: 0.6248\n",
      "Epoch 11/20\n",
      "3504/3504 [==============================] - 1s 182us/step - loss: 0.8969 - acc: 0.7217 - val_loss: 1.1233 - val_acc: 0.6401\n",
      "Epoch 12/20\n",
      "3504/3504 [==============================] - 1s 181us/step - loss: 0.8313 - acc: 0.7403 - val_loss: 0.9652 - val_acc: 0.6900\n",
      "Epoch 13/20\n",
      "3504/3504 [==============================] - 1s 190us/step - loss: 0.7775 - acc: 0.7566 - val_loss: 1.0522 - val_acc: 0.6886\n",
      "Epoch 14/20\n",
      "3504/3504 [==============================] - 1s 186us/step - loss: 0.7410 - acc: 0.7646 - val_loss: 1.0062 - val_acc: 0.6773\n",
      "Epoch 15/20\n",
      "3504/3504 [==============================] - 1s 179us/step - loss: 0.7475 - acc: 0.7631 - val_loss: 0.9925 - val_acc: 0.6660\n",
      "Train on 3504 samples, validate on 1503 samples\n",
      "Epoch 1/20\n",
      "3504/3504 [==============================] - 2s 471us/step - loss: 1.2907 - acc: 0.6213 - val_loss: 0.9476 - val_acc: 0.6866\n",
      "Epoch 2/20\n",
      "3504/3504 [==============================] - 1s 228us/step - loss: 1.0469 - acc: 0.6544 - val_loss: 0.9395 - val_acc: 0.6700\n",
      "Epoch 3/20\n",
      "3504/3504 [==============================] - 1s 213us/step - loss: 0.9128 - acc: 0.6738 - val_loss: 0.8802 - val_acc: 0.6873\n",
      "Epoch 4/20\n",
      "3504/3504 [==============================] - 1s 225us/step - loss: 0.8606 - acc: 0.6946 - val_loss: 0.9157 - val_acc: 0.7019\n",
      "Epoch 5/20\n",
      "3504/3504 [==============================] - 1s 223us/step - loss: 0.8208 - acc: 0.7058 - val_loss: 0.8765 - val_acc: 0.7146\n",
      "Epoch 6/20\n",
      "3504/3504 [==============================] - 1s 215us/step - loss: 0.7769 - acc: 0.7138 - val_loss: 0.8221 - val_acc: 0.7073\n",
      "Epoch 7/20\n",
      "3504/3504 [==============================] - 1s 212us/step - loss: 0.7333 - acc: 0.7349 - val_loss: 0.8080 - val_acc: 0.7033\n",
      "Epoch 8/20\n",
      "3504/3504 [==============================] - 1s 240us/step - loss: 0.6996 - acc: 0.7477 - val_loss: 0.8335 - val_acc: 0.7079\n",
      "Epoch 9/20\n",
      "3504/3504 [==============================] - 1s 213us/step - loss: 0.6745 - acc: 0.7463 - val_loss: 0.8539 - val_acc: 0.7146\n",
      "Epoch 10/20\n",
      "3504/3504 [==============================] - 1s 226us/step - loss: 0.6532 - acc: 0.7577 - val_loss: 0.7928 - val_acc: 0.7219\n",
      "Epoch 11/20\n",
      "3504/3504 [==============================] - 1s 228us/step - loss: 0.6265 - acc: 0.7686 - val_loss: 0.8050 - val_acc: 0.7219\n",
      "Epoch 12/20\n",
      "3504/3504 [==============================] - 1s 239us/step - loss: 0.6113 - acc: 0.7717 - val_loss: 0.7952 - val_acc: 0.7199\n",
      "Epoch 13/20\n",
      "3504/3504 [==============================] - 1s 214us/step - loss: 0.5988 - acc: 0.7780 - val_loss: 0.7849 - val_acc: 0.7292\n",
      "Epoch 14/20\n",
      "3504/3504 [==============================] - 1s 223us/step - loss: 0.5681 - acc: 0.7891 - val_loss: 0.8094 - val_acc: 0.7166\n",
      "Epoch 15/20\n",
      "3504/3504 [==============================] - ETA: 0s - loss: 0.5428 - acc: 0.801 - 1s 223us/step - loss: 0.5428 - acc: 0.8008 - val_loss: 0.8184 - val_acc: 0.7332\n",
      "Epoch 16/20\n",
      "3504/3504 [==============================] - 1s 227us/step - loss: 0.5123 - acc: 0.8119 - val_loss: 0.8197 - val_acc: 0.7279\n",
      "Train on 3504 samples, validate on 1503 samples\n",
      "Epoch 1/20\n",
      "3504/3504 [==============================] - 2s 542us/step - loss: 1.0844 - acc: 0.6458 - val_loss: 0.9119 - val_acc: 0.6713\n",
      "Epoch 2/20\n",
      "3504/3504 [==============================] - 1s 247us/step - loss: 0.9389 - acc: 0.6635 - val_loss: 0.9137 - val_acc: 0.6786\n",
      "Epoch 3/20\n",
      "3504/3504 [==============================] - 1s 240us/step - loss: 0.9022 - acc: 0.6755 - val_loss: 0.8548 - val_acc: 0.6860\n",
      "Epoch 4/20\n",
      "3504/3504 [==============================] - 1s 237us/step - loss: 0.8620 - acc: 0.6846 - val_loss: 0.8510 - val_acc: 0.6880\n",
      "Epoch 5/20\n",
      "3504/3504 [==============================] - 1s 231us/step - loss: 0.8581 - acc: 0.6844 - val_loss: 0.8449 - val_acc: 0.6986\n",
      "Epoch 6/20\n",
      "3504/3504 [==============================] - 1s 230us/step - loss: 0.8254 - acc: 0.6909 - val_loss: 0.8519 - val_acc: 0.6999\n",
      "Epoch 7/20\n",
      "3504/3504 [==============================] - 1s 227us/step - loss: 0.8333 - acc: 0.6946 - val_loss: 0.8125 - val_acc: 0.7033\n",
      "Epoch 8/20\n",
      "3504/3504 [==============================] - 1s 230us/step - loss: 0.7946 - acc: 0.7158 - val_loss: 0.8159 - val_acc: 0.7119\n",
      "Epoch 9/20\n",
      "3504/3504 [==============================] - 1s 232us/step - loss: 0.7691 - acc: 0.7143 - val_loss: 0.8162 - val_acc: 0.7159\n",
      "Epoch 10/20\n",
      "3504/3504 [==============================] - 1s 228us/step - loss: 0.7370 - acc: 0.7292 - val_loss: 0.8493 - val_acc: 0.7186\n",
      "Train on 3504 samples, validate on 1503 samples\n",
      "Epoch 1/20\n",
      "3504/3504 [==============================] - 2s 589us/step - loss: 1.0850 - acc: 0.6436 - val_loss: 0.9066 - val_acc: 0.6840\n",
      "Epoch 2/20\n",
      "3504/3504 [==============================] - 1s 249us/step - loss: 0.9294 - acc: 0.6675 - val_loss: 0.9006 - val_acc: 0.6880\n",
      "Epoch 3/20\n",
      "3504/3504 [==============================] - 1s 245us/step - loss: 0.8872 - acc: 0.6789 - val_loss: 0.8504 - val_acc: 0.7019\n",
      "Epoch 4/20\n",
      "3504/3504 [==============================] - 1s 242us/step - loss: 0.8527 - acc: 0.6901 - val_loss: 0.8435 - val_acc: 0.7079\n",
      "Epoch 5/20\n",
      "3504/3504 [==============================] - 1s 253us/step - loss: 0.8134 - acc: 0.7018 - val_loss: 0.8394 - val_acc: 0.7099\n",
      "Epoch 6/20\n",
      "3504/3504 [==============================] - 1s 249us/step - loss: 0.7879 - acc: 0.7138 - val_loss: 0.8061 - val_acc: 0.7192\n",
      "Epoch 7/20\n",
      "3504/3504 [==============================] - 1s 241us/step - loss: 0.7722 - acc: 0.7246 - val_loss: 0.8195 - val_acc: 0.7019\n",
      "Epoch 8/20\n",
      "3504/3504 [==============================] - 1s 247us/step - loss: 0.7294 - acc: 0.7337 - val_loss: 0.8034 - val_acc: 0.7159\n",
      "Epoch 9/20\n",
      "3504/3504 [==============================] - 1s 248us/step - loss: 0.7177 - acc: 0.7323 - val_loss: 0.8023 - val_acc: 0.7272\n",
      "Epoch 10/20\n",
      "3504/3504 [==============================] - 1s 247us/step - loss: 0.7059 - acc: 0.7400 - val_loss: 0.7837 - val_acc: 0.7319\n",
      "Epoch 11/20\n",
      "3504/3504 [==============================] - 1s 254us/step - loss: 0.6769 - acc: 0.7474 - val_loss: 0.8196 - val_acc: 0.7239\n",
      "Epoch 12/20\n",
      "3504/3504 [==============================] - 1s 250us/step - loss: 0.6580 - acc: 0.7600 - val_loss: 0.8034 - val_acc: 0.7226\n",
      "Epoch 13/20\n",
      "3504/3504 [==============================] - 1s 247us/step - loss: 0.6355 - acc: 0.7688 - val_loss: 0.8374 - val_acc: 0.7199\n"
     ]
    }
   ],
   "source": [
    "nn_fit_start = time.time()\n",
    "nn_model_1 = create_seq_model(1, 30)\n",
    "nn_model_2 = create_seq_model(3, 85)\n",
    "nn_model_3 = create_seq_model(5, 70)\n",
    "nn_model_4 = create_seq_model(5, 150)\n",
    "nn_fit_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential NN fit time(seconds):  47.84800863265991\n"
     ]
    }
   ],
   "source": [
    "nn_fit_time = nn_fit_end - nn_fit_start\n",
    "print('Sequential NN fit time(seconds): ', nn_fit_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple neuron network of 5 layers and 150 nodes per layer seems to performing in a manner satisfactory manner and seems to be very fast to tune with a dedicated GPU. But generally all networks performed quite similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7182507987220448\n",
      "\n",
      "[[  37   36   53    0   23    7    1]\n",
      " [  32   92   64    1   66    1    3]\n",
      " [  26   25  270    0  190   23    3]\n",
      " [   4   19   20    0   18    2    1]\n",
      " [   7   31  145    1 3138   28    6]\n",
      " [   5   13  165    0  316   54    8]\n",
      " [   6   12    4    0   44    2    6]]\n",
      "\n",
      "f1 average:  0.6817478659478878\n",
      "f1:  [0.27007299 0.37782341 0.42925278 0.         0.87763949 0.15929204\n",
      " 0.11764706]\n",
      "Sequential NN prediction time(seconds):  0.7041149139404297\n"
     ]
    }
   ],
   "source": [
    "# carry prediction with time measurements \n",
    "# while recording prediction\n",
    "\n",
    "nn_pred_start = time.time()\n",
    "prediction = nn_model_4.predict_classes(X_test)\n",
    "nn_pred_end = time.time()\n",
    "\n",
    "nn_met = printMetrics(prediction, y_test)\n",
    "nn_pred_time = nn_pred_end - nn_pred_start\n",
    "print('Sequential NN prediction time(seconds): ', nn_pred_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the a sequential neural network doesn't actually perform better than logistic regression while being slightly slower to fit. Perhaps an approach were we do not look at the picture pixel by pixel would help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Model Assessment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a simple sequential neural network doesn't cut it for this dataset still, this is probably because of the way the data is handled. The neural network still has to work in a pixel by pixel manner due to the dataset and its nature. Perhaps a convolutional neural network can resolve this by using kernels to identify features instead of thinking a pixel by pixel manner, however this will need the pipeline to be reorganised to load the images instead of pixel columns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
